{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Dropout,TimeDistributed\n",
    "from keras.layers import LSTM,SimpleRNN\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os, os.path\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from unicodedata import normalize\n",
    "import codecs, io\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- ### Check GPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'/gpu:0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Check and set Twitter's API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "api = twitter.Api(consumer_key='LyNVanTEQEOEGKfXAMeLv6AKG',\n",
    "                    consumer_secret='0lJvhaaOP5cRZWm6rxwyBIAypd1P7eiDx9f74KBDlLrSldNuBQ',\n",
    "                    access_token_key='855852332034265088-geTEVmA7xIsOD3WCZyfBNnqjRdS1MhW',\n",
    "                    access_token_secret='kJMwMl67e3nYrqaGWzIizxzQpRZhtBfOnwPflO1fk3cOt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"created_at\": \"Sat Apr 22 18:34:31 +0000 2017\", \"default_profile\": true, \"description\": \"Learning how to be creative\", \"followers_count\": 2, \"friends_count\": 1, \"id\": 855852332034265088, \"lang\": \"en\", \"location\": \"Somewhere in the cloud\", \"name\": \"ArtistBot\", \"profile_background_color\": \"F5F8FA\", \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/855852332034265088/1492892354\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/855878764143804417/r55Z2Js5_normal.jpg\", \"profile_link_color\": \"1DA1F2\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"profile_text_color\": \"333333\", \"screen_name\": \"TheTalkativeBot\", \"status\": {\"created_at\": \"Tue Apr 25 22:31:31 +0000 2017\", \"id\": 856999137753145349, \"id_str\": \"856999137753145349\", \"in_reply_to_screen_name\": \"TheTalkativeBot\", \"in_reply_to_status_id\": 856894659007844352, \"in_reply_to_user_id\": 855852332034265088, \"lang\": \"en\", \"media\": [{\"display_url\": \"pic.twitter.com/lMV1EeegY0\", \"expanded_url\": \"https://twitter.com/TheTalkativeBot/status/856999137753145349/photo/1\", \"id\": 856999135043620864, \"media_url\": \"http://pbs.twimg.com/media/C-SsLDkXkAAvT-S.jpg\", \"media_url_https\": \"https://pbs.twimg.com/media/C-SsLDkXkAAvT-S.jpg\", \"sizes\": {\"large\": {\"h\": 1200, \"resize\": \"fit\", \"w\": 900}, \"medium\": {\"h\": 1200, \"resize\": \"fit\", \"w\": 900}, \"small\": {\"h\": 680, \"resize\": \"fit\", \"w\": 510}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"type\": \"photo\", \"url\": \"https://t.co/lMV1EeegY0\"}], \"source\": \"<a href=\\\"http://www.google.com\\\" rel=\\\"nofollow\\\">TheScenarioBot</a>\", \"text\": \"@dvp_tran Here what's your picture evokes to me ! https://t.co/lMV1EeegY0\"}, \"statuses_count\": 41}\n"
     ]
    }
   ],
   "source": [
    "print(api.VerifyCredentials())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "api.PostUpdate(\"Today I am learning German, watch me improve! ;) \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Learning from corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1. Load and convert data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Example script to generate text from Nietzsche's writings.\n",
    "At least 20 epochs are required before the generated text\n",
    "starts sounding coherent.\n",
    "It is recommended to run this script on GPU, as recurrent\n",
    "networks are quite computationally intensive.\n",
    "If you try this script on new data, make sure your corpus\n",
    "has at least ~100k characters. ~1M is better.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Load and concatenate files:\n",
    "\n",
    "DIR=\"../../LSTM/data/Gutenberg/ebooks-unzipped/French/\"\n",
    "all_files = [f for f in listdir(DIR) if isfile(join(DIR, f))]\n",
    "\n",
    "#choose how many files to concatenate:\n",
    "nb_files=7\n",
    "if nb_files>len(all_files):\n",
    "    nb_files=len(all_files)\n",
    "    \n",
    "    \n",
    "out_path=\"french/data/\"\n",
    "if not os.path.exists(out_path):\n",
    "    os.makedirs(out_path)\n",
    "if not os.path.exists(out_path+\"input/\"):\n",
    "    os.makedirs(out_path+\"input/\")\n",
    "    \n",
    "with open(out_path+'input/french2.txt', 'w') as outfile:\n",
    "    for fname in all_files[0:nb_files]:\n",
    "        with open(DIR+fname) as infile:\n",
    "            i=0\n",
    "            for line in infile:\n",
    "                if i>=50:\n",
    "                    outfile.write(line)\n",
    "                i=i+1\n",
    "        print (\"Done concatenating file : %s\" %fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done concatenating file : 249.txt\n",
      "Done concatenating file : 4740-8.txt\n",
      "Done concatenating file : 799-0.txt\n",
      "Done concatenating file : 4548-8.txt\n",
      "Done concatenating file : 4791-8.txt\n",
      "Done concatenating file : 803-8.txt\n",
      "Done concatenating file : 2650-0.txt\n"
     ]
    }
   ],
   "source": [
    "#Load and concatenate files:\n",
    "\n",
    "DIR=\"../../LSTM/data/Gutenberg/ebooks-unzipped/French/\"\n",
    "all_files = [f for f in listdir(DIR) if isfile(join(DIR, f))]\n",
    "\n",
    "#choose how many files to concatenate:\n",
    "nb_files=7\n",
    "if nb_files>len(all_files):\n",
    "    nb_files=len(all_files)\n",
    "    \n",
    "    \n",
    "out_path=\"french/data/\"\n",
    "if not os.path.exists(out_path):\n",
    "    os.makedirs(out_path)\n",
    "if not os.path.exists(out_path+\"input/\"):\n",
    "    os.makedirs(out_path+\"input/\")\n",
    "    \n",
    "with io.open(out_path+'input/french2.txt', 'w', encoding='utf-8', errors='replace')  as outfile:\n",
    "    for fname in all_files[0:nb_files]:\n",
    "        with io.open(DIR+fname,'r',encoding='utf-8',errors=\"replace\") as infile:\n",
    "            i=0\n",
    "            for line in infile:\n",
    "                if i>=50:\n",
    "                    outfile.write(line)\n",
    "                i=i+1\n",
    "        print (\"Done concatenating file : %s\" %fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UTF-8'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ISO-8859-1\"\n",
    "\"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load file\n",
    "file_name=out_path+'input/french2.txt'\n",
    "text = open(file_name).read()\n",
    "text=normalize('NFKD',text.decode('latin1')).encode('ASCII', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = re.sub(\"\\n\\n+\" , \"\\n\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 2504261\n",
      "total chars: 88\n"
     ]
    }
   ],
   "source": [
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "VOCAB_SIZE = len(chars)\n",
    "print('total chars:',VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** The RNN takes in input numerical data hence the necessity to convert strings into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating mapping between indexes and characters\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weâ€™re gonna use Keras to create and train our Network, so we must convert the data into this form: (number_of_sequences, length_of_sequence, number_of_features).\n",
    "- nb of features = length of the char array\n",
    "- length of sequence = batch size\n",
    "- nb of sequence = len(data) divided by batch size.\n",
    "\n",
    "**Warning : ** target sequence is setted by shifting the source/input sequence by one character with both having the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.94 s, sys: 152 ms, total: 2.1 s\n",
      "Wall time: 2.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "SEQ_LENGTH=100\n",
    "#Build three dimensional arrays\n",
    "X = np.zeros((len(text)/SEQ_LENGTH, SEQ_LENGTH, VOCAB_SIZE)) #input\n",
    "y = np.zeros((len(text)/SEQ_LENGTH, SEQ_LENGTH, VOCAB_SIZE)) #target\n",
    "\n",
    "#Build sequences\n",
    "for i in range(0, len(text)/SEQ_LENGTH):\n",
    "    X_sequence = text[i*SEQ_LENGTH:(i+1)*SEQ_LENGTH]\n",
    "    X_sequence_ix = [char_indices[value] for value in X_sequence]\n",
    "    input_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n",
    "    for j in range(SEQ_LENGTH):\n",
    "        input_sequence[j][X_sequence_ix[j]] = 1.\n",
    "    X[i] = input_sequence\n",
    "\n",
    "    y_sequence = text[i*SEQ_LENGTH+1:(i+1)*SEQ_LENGTH+1]\n",
    "    y_sequence_ix = [char_indices[value] for value in y_sequence]\n",
    "    target_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n",
    "    for j in range(SEQ_LENGTH):\n",
    "        target_sequence[j][y_sequence_ix[j]] = 1.\n",
    "    y[i] = target_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2. Build the network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HIDDEN_DIM= 500 #500\n",
    "LAYER_NUM = 2\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(LAYER_NUM - 1):\n",
    "    model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_text(model, length, vocab_size, ix_to_char):\n",
    "    # starting with random character\n",
    "    ix = [np.random.randint(vocab_size)]\n",
    "    y_char = [ix_to_char[ix[-1]]]\n",
    "    X = np.zeros((1, length, vocab_size))\n",
    "    for i in range(length):\n",
    "        # appending the last predicted character to sequence\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        print(ix_to_char[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(ix_to_char[ix[-1]])\n",
    "    return ('').join(y_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".qx4!!::BYYDDDDDDdDDDDDYDDDDDDYDDDDDD]]dYDDDDDDYDDDDDYDDDDDD]]dYDDDDDDYDDDDDYDDDDDD]]dYDDDDDDYDDDDDY"
     ]
    }
   ],
   "source": [
    "# Generate some sample before training to know how bad it is!\n",
    "bla = generate_text(model, 100, VOCAB_SIZE, indices_char)\n",
    "#api.PostUpdate(status=bla[0:123])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Train network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_iternb(string):\n",
    "    return re.findall(r'checkpoint_500_epoch_(.*).hdf5', string)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at iteration : 300\n",
      "\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "84s - loss: 0.1835\n",
      "---\n",
      "                                                                                                                                        \n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "82s - loss: 0.1560\n",
      "+ Bruce, Barbicane et Michel Ardan, il m'arriva A \n",
      "enveloppe la loueur du son lur ou non, amA ne trois fois. A vais permirent, tout espAra\n",
      "u\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "81s - loss: 0.1516\n",
      "?  Par un procAdA bien simple, celui de MM.\n",
      "Reiset et Regnault, indiquA par Michel Ardan en entrant successivement chez\n",
      "lui, seule avant de \n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "81s - loss: 0.1504\n",
      "fait d'abord en tous\n",
      "les trois i12 quelques bras sans explosions.\n",
      "--Bon!  ri12pondit le docteur, si je suis sAre que j'ai soulevi12 le sol p\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "81s - loss: 0.1427\n",
      "bliais le passi12, je di12daignais l'avenir.  Rien n'existe\n",
      "ni12cessaire, on peut admet rien, pas dix bonsoirs ses regards.\n",
      "Le sand et soi d\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1425\n",
      "bien de mettre A  tout\n",
      "hasard dans ta malle ton pardessus ces ouvers me faisaient dire i12 tous moments au\n",
      "fagerle de sa souffrance; ce qui \n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1438\n",
      "z puissant pour amortir le choc, et, pendant sa fameuse promenade\n",
      "dans le bois de Skersnaw, il avait ces rasensons entrecasAs des ressemblai\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1445\n",
      "A  la bAnAdre durane de soie que\n",
      "laisser et souvent des hA telA se direction par sa fille, quand il pensait\n",
      "une heure anormale pour aller ch\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1417\n",
      "\u0019 Saint-Euverte, parce qu'il lui\n",
      "rApondait A  un dire que, dans une mAdacure dont le plus\n",
      "puissant apparemistrait,--comme les millions de de\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1430\n",
      "Web site includes information about Project Gutenberg-tm,\n",
      "including how to make donations to the Project Gutenberg Literary Archive Foundati\n",
      "\n",
      "Iteration nb : 310\n",
      "\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1410\n",
      "90\n",
      "centimA tres dans la premiA re seconde; A  la distance oA1 se trouvait\n",
      "prAats, pour avoir rendue de chemin du sein de l'interroger vapace\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1385\n",
      "Kennedy, si nous continuons i12 produire de\n",
      "pareils effets, nous aurons de la peine i12 i12tablir des formes\n",
      "de phrasi12me. Les harmoniers, \n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1384\n",
      "% Zambourg.\n",
      "Nous sommes enfermi12s par un vieil i12voli12 de toiles; de l'abondole n'existait peut-i12tre pas de\n",
      "longues aigre; mais il refr\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1351\n",
      "Roussainville dans l'Apaisseur desquels nous pourrions nous\n",
      "mettre A  couvert.\n",
      "Souvent plus lettres i12 leurs di12penses i12 peine enchante.\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1364\n",
      "s de chose mangeable, ou de\n",
      "tendre embellissement A  une toilette pour une recherche bien des amis dont\n",
      "ses deux voyants empresezent l'horiz\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1407\n",
      "*\n",
      "Sei12tre fit un jour, le 30 jandisien, bonne nous abandonnent au\n",
      "chasseur qui ne sont ou moins redoubli12 ces caracti12res fi12roces. Ce f\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1371\n",
      "Y OR FITNESS FOR ANY PURPOSE.\n",
      "1.F.5.  Some states do not allow disclaimers of certain implied\n",
      "warranties or the exclusion or limitation of c\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1359\n",
      "Qui sait, Joe?\n",
      "--Ah! si vous faites une chose pareille, je vous proclame le profit\n",
      "d'accuser tout i12 bout.  Pourquoi?  Il y mit i12 mon gra\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1378\n",
      "\\FILLE TO BALION, OUHEDRE FITNENTYTLES, WORCRE WARRANNIARE ***\n",
      "***** This file should be named 4740-8.txt or 4740-8.zip *****\n",
      "This and all a\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1299\n",
      "Bahama signalA rent une Apaisse fumAe A  l'horizon.  Deux heures plus\n",
      "tard, un grand manue de fAate sans tAmoine en nous que j'allais\n",
      "veille\n",
      "\n",
      "Iteration nb : 320\n",
      "\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1366\n",
      "Bade jusqu'A  six cents pieds dans le sol!  Eh bien!\n",
      "de quoi s'agissait-il, en somme?  De tripler cet on peut Aatre,\n",
      "s'est-ce que cela, ci..\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1381\n",
      "Project Gutenberg-tm trademark as set forth in paragraphs 1.E.8 or\n",
      "1.E.9.\n",
      "1.E.3.  If an individual Project Gutenberg-tm electronic work is p\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1345\n",
      "ce monde.  Aussi pas un curieux qui ne\n",
      "voulAt se donner la jouissance de visiter intArieurement cet amour par\n",
      "un but en redre dans le pays d\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1268\n",
      "Kennedy. Ce n'est qu'un\n",
      "affaiblissement passager. Vous ne mourrez pas! Peut-on mourir par\n",
      "cette belle de chasse avec celles de l'i12poque de\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1373\n",
      "\n",
      "les fleurs n'avaient rien de plus possible; les pri12paratifs du volcans ond\n",
      "l'autre avait une belle tain pour pouvait faire un\n",
      "poids de ci\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1318\n",
      "Et les jours oA1 par hasard elle avait encore\n",
      "AtA gentille et tendre avec lui, si elle avait eu quelquefois de son visage,\n",
      "le terre, comme s\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1254\n",
      "6 D0 CF C5_BC_  _BC_D0 AD A6 E6 E6 B3\n",
      "    C5 D8 CF B3 D0 C5_C1_   B3 A2 D0 C5 B4 CF       E6 E6 C1 DA CF BC C_ ADAACD        ___       A2 i \n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1357\n",
      "\\PITRE XXX\n",
      "Le ciel dispusaient. Ces paroles nourrirs manqueraient sans qu'il s'en fallaient\n",
      "quand elle faisait tout ce qu'il avait disposA c\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1336\n",
      "9.\n",
      "1.E.3.  If an individual Project Gutenberg-tm electronic work is derived\n",
      "from the public domain (does not contain a notice indicating tha\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1331\n",
      ": AC'est la petite phrase de\n",
      "la sonate de Vinteuil, n'Acoutons pas!A tous ses souvenirs du temps oA1\n",
      "ma tante n'Atait pas trA s prAsentA, Ac\n",
      "\n",
      "Iteration nb : 330\n",
      "\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1336\n",
      "ks by freely sharing Project Gutenberg-tm\n",
      "works in compliance with the terms of this agreement for keeping the\n",
      "Project Gutenberg-tm name ass\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1347\n",
      "ue se rApandAt et\n",
      "s'AvaporAt sa vertu volatile et, justement ces soirs-lA  oA1 j'aurais eu\n",
      "besoin de le faire descendre davantage. Cela me r\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1321\n",
      "Vinteuil, A  qui, jusque-lA  il n'adressait pas\n",
      "la parole, et lui avait demandA avant de nous quitter, A  la dArive, comme\n",
      "aprA s l'ordre d'\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1311\n",
      "* Swann sentait sans trop et\n",
      "sincer A  sa surface aux portures de la monotone de mes regards.\n",
      "Sa femme la gutela plus agrAable, des brumes e\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1350\n",
      "La cuisine de Joe.--Dissertation sur la vianue\n",
      "si12rie?... i12 cette halte qui le rei12ure i12 une hauteur de deux mille\n",
      "cinq cents milles d\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1338\n",
      "Dick.\n",
      "--Et cela ne t'inquii12te pas un peu?\n",
      "--Pourquoi?\n",
      "--C'est que ce chemin-li12 nous mi12ne i12 Tramps, et trop violemment, accrochi12 so\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1300\n",
      "!  criait l'un des plus grands chagrins qu'il m'a\n",
      "dit que tu n'as pas trop de la mien?  que j'Atais assez beaucoup\n",
      "plus loin, l'apparence es\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1311\n",
      "je cherchais A \n",
      "m'Atats des phrases. Elle ajouta avec lui dAplacer que le jour s'Atendit dans mes\n",
      "sangages qui avaient pour entendre une rAc\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1341\n",
      "Royalty payments should be clearly marked as such and\n",
      "     sent to the Project Gutenberg Literary Archive Foundation at the\n",
      "     address spe\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1303\n",
      "4.  Except for the limited right of replacement or refund set forth\n",
      "in paragraph 1.F.3, this work is provided to you 'AS-IS', WITH NO\n",
      "OTHER \n",
      "\n",
      "Iteration nb : 340\n",
      "\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1343\n",
      "jours. Comme les diffArents malades, Aregains\n",
      "prA s d'une fois arrivA au sor sur laquelle les autres\n",
      "filaies comme dans un bond qu'on avait \n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1316\n",
      "4.  Except for the limited right of replacement or refund set forth\n",
      "in paragraph 1.F.3, this work is provided to you 'AS-IS'of Em Hureu, Co\n",
      "\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1304\n",
      ") et dont la nuit sitA t les aveugles de\n",
      "ventes ou d'acrA s-lA . Ah! Mars, Diane se perguait en ce retonnant des commers impAnAsuvres\n",
      "que ri\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1350\n",
      "re de la rampe de la prominie: par le Victoria pendant les groupes\n",
      "du Juri12, de Si12nieds, de toutes les endroits les plus abrures\n",
      "assez ri\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1311\n",
      " de l'i12corce terrestre.\n",
      "--Je ne dis pas non.\n",
      "--Et ici, suivant la loi de l'accroissement de la tempi12rature\n",
      "qui nous recevoir.\n",
      "i12Mainten\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1295\n",
      "A  un de ses amis aussi A \n",
      "l'AtablitA du regard.\n",
      "Mais les choses sont plus effet qu'il lui semblait malin. Et\n",
      "comme un marin morceau s'il s'\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1267\n",
      "me cela.A Et il y a quelques mots comme s'Atait-il\n",
      "passA le peintre qui m'emplacerait. Elle revenait en toute\n",
      "monstre de plus en plus mystAr\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1344\n",
      "Web site includes information about Project Gutenberg-tm,\n",
      "including how to make donations to the Project Gutenberg Literary\n",
      "Archive Foundati\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1312\n",
      "Qui sait, Joe?\n",
      "--Ah! si vous faites une chose pareille, je vous proclame le\n",
      "premier savant du monde. Ne troupes que le jour\n",
      "impossible! Mon \n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1331\n",
      "\u0019 Guermantes, on\n",
      "me semble prAate A  regarder d'un seul moyen qu'il aime morte au planteur du\n",
      "Soleil, et au milieu des airs et des choses qu\n",
      "\n",
      "Iteration nb : 350\n",
      "\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1320\n",
      "re A  sa parente ou amie, il nous assure que rien\n",
      "n'est plus simple, nous fait entrer dans le vestibu?i\n",
      "m'incomprAhense la heure, dArais plu\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1325\n",
      "Qui ei12t jamais imagini12\n",
      "dans cette i12corce terrestre un oci12an vi12ritable, avec sentir\n",
      "ce qu'il faut, l'aimait d'une voliation de prem\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "84s - loss: 0.1320\n",
      "000 livres\n",
      "Tel i12tait le di12compte des quatre mille livres que le docteur\n",
      "Fergusson se proposait d'une autre question, un moment accord\n",
      "pe\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "80s - loss: 0.1327\n",
      "Bahama signalA rent une Apaisse de parties violettes\n",
      "et comme en ces termes:\n",
      "--Avec ses regards chargAes ne se refusernaient plus. Et main\n",
      "e\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "92s - loss: 0.1322\n",
      "______________________\n",
      "Di12sir au pied du sol\n",
      "en ri12galait dernii12\n",
      "une personne que nous i12tions partissait ennuit dans l'heure\n",
      "et de par\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "97s - loss: 0.1315\n",
      "(et quand bien mAame des hA tels s'y superposeraient maintenant\n",
      "sans pouvoir y dAner une vAritable joie A  lui faire des matinales de ma gra\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "90s - loss: 0.1281\n",
      "Royalty payments should be clearly marked as such and\n",
      "     sent to the Project Gutenberg Literary Archive Foundation at the\n",
      "     address spe\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "90s - loss: 0.1314\n",
      "4.  Except for the limited right of replacement or refund set forth\n",
      "in paragraph 1.F.3, this work is any propertible to the exceval il way u\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "88s - loss: 0.1308\n",
      "nt de Mme\n",
      "Verdurin (Ata-t-il doute il faut compter, je sentais que je n'avais\n",
      "pas atte de beaucoup d'Achauffer dans ce cas d'AcurisAe dont i\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "88s - loss: 0.1305\n",
      "Bah! Monsieur, il n'y a qu'i12 suivre les i12vitations.\n",
      "--Et que nous manque-t-il faudre se pri12senter i12 cette terrible question.  Le rad\n",
      "\n",
      "Iteration nb : 360\n",
      "\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "85s - loss: 0.1304\n",
      "\"[INDE NOTY PUKS BROTATRACE CEN QUECLICILEMNETY, IFIKENS *****\n",
      "******\n",
      "S ***** **** TRI.\n",
      "Le lendemain je le disais.\n",
      "--Non!  ri12pondit le doc\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "85s - loss: 0.1297\n",
      "------------------------------\n",
      "\n",
      "                                                                                                            \n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "93s - loss: 0.1315\n",
      "This and all associated files of various formats will be found in:\n",
      "        http://www.gutenberg.org/4/7/4/4740/\n",
      "Updated edition\n",
      "with the phr\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "83s - loss: 0.1306\n",
      "Hier soir, s'Acria J.-T. Maston _ex abrupto_, notre prAsident a AtA\n",
      "indiscutivement que le nom de Michel Ardan qui ne voulait pas recevoir s\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "90s - loss: 0.1320\n",
      "Santa-Anna!  Un pays enfin qui se reflAtait par\n",
      "mille pieds cubes, ou trois ou qui nous dAjeviraient,\n",
      "comme si elles avaient AtA trA s aimab\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "#batch size equals to seq length here\n",
    "BATCH_SIZE=300\n",
    "#len of desired output\n",
    "GENERATE_LENGTH=140\n",
    "DIR=out_path+\"weights/weight_attempt_s03_2/\"\n",
    "flag=True\n",
    "\n",
    "try:\n",
    "    onlyfiles = [f for f in listdir(DIR) if isfile(join(DIR, f))]\n",
    "    iteration=[]\n",
    "    for files in onlyfiles:\n",
    "        iteration.append(int(get_iternb(files)))\n",
    "    iteration=max(iteration)\n",
    "\n",
    "    last_checkpoint=DIR+onlyfiles[0][0:21]+str(iteration)+'.hdf5'\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    onlyfiles=[]\n",
    "    if not os.path.exists(DIR):\n",
    "        os.makedirs(DIR)\n",
    "    nb_files=0\n",
    "\n",
    "if nb_files>0:\n",
    "    model.load_weights(last_checkpoint)\n",
    "else:\n",
    "    iteration=0\n",
    "    \n",
    "print(\"Starting at iteration : %s\" %iteration)\n",
    "while flag==True:\n",
    "    print('\\n')\n",
    "    print('-'*20)\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, verbose=2, nb_epoch=1)\n",
    "    iteration += 1\n",
    "    bla=generate_text(model, GENERATE_LENGTH,VOCAB_SIZE, indices_char)\n",
    "    if iteration % 10 == 0:\n",
    "        print(\"\\n\\nIteration nb : %s\" %iteration)\n",
    "        #api.PostUpdate(status=bla[0:123])\n",
    "        model.save_weights(DIR+'checkpoint_{}_epoch_{}.hdf5'.format(HIDDEN_DIM, iteration))\n",
    "        #remove unecessary files:\n",
    "        for files in onlyfiles:\n",
    "            try:\n",
    "                if files:\n",
    "                    os.remove(DIR+files)\n",
    "            except:\n",
    "                pass\n",
    "        onlyfiles = [f for f in listdir(DIR) if isfile(join(DIR, f))]\n",
    "\n",
    "    if iteration>=900:\n",
    "        print(\"Stopping...\")\n",
    "        flag=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Generate text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_text(model, length, vocab_size, ix_to_char):\n",
    "    # starting with random character\n",
    "    ix = [np.random.randint(vocab_size)]\n",
    "    y_char = [ix_to_char[ix[-1]]]\n",
    "    X = np.zeros((1, length, vocab_size))\n",
    "    for i in range(length):\n",
    "        # appending the last predicted character to sequence\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        print(ix_to_char[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(ix_to_char[ix[-1]])\n",
    "    if not os.path.exists(out_path+\"generate/\"):\n",
    "        os.makedirs(out_path+\"generate/\")\n",
    "    with open(out_path+\"generate/output.txt\",\"w\") as f:\n",
    "        f.write(('').join(y_char))\n",
    "    return ('').join(y_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#seed with particular text:\n",
    "def generate_text_seeded(model,seed,length, vocab_size, ix_to_char):\n",
    "    # starting with random character\n",
    "    # char_indices\n",
    "    ix = [char_indices[x] for x in seed]\n",
    "    y_char = [x for x in seed]\n",
    "    X = np.zeros((1, length, vocab_size))\n",
    "    for i in range(len(ix)) :\n",
    "        X[0, i, :][ix[i]] = 1\n",
    "        print(ix_to_char[ix[i]], end=\"\")\n",
    "    to_substract = len(ix)\n",
    "    for i in range(length-to_substract):\n",
    "        # appending the last predicted character to sequence\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        print(ix_to_char[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(ix_to_char[ix[-1]])\n",
    "    return ('').join(y_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_text_seeded(model,normalize('NFKD',\"le chien \".decode('latin1')), 1000, VOCAB_SIZE, indices_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "out = save_text(model, 1500, VOCAB_SIZE, indices_char)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
