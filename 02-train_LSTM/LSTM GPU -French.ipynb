{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Dropout,TimeDistributed\n",
    "from keras.layers import LSTM,SimpleRNN\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os, os.path\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from unicodedata import normalize\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- ### Check GPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'/gpu:0']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Check and set Twitter's API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "api = twitter.Api(consumer_key='LyNVanTEQEOEGKfXAMeLv6AKG',\n",
    "                    consumer_secret='0lJvhaaOP5cRZWm6rxwyBIAypd1P7eiDx9f74KBDlLrSldNuBQ',\n",
    "                    access_token_key='855852332034265088-geTEVmA7xIsOD3WCZyfBNnqjRdS1MhW',\n",
    "                    access_token_secret='kJMwMl67e3nYrqaGWzIizxzQpRZhtBfOnwPflO1fk3cOt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"created_at\": \"Sat Apr 22 18:34:31 +0000 2017\", \"default_profile\": true, \"description\": \"Learning how to be creative\", \"followers_count\": 2, \"friends_count\": 1, \"id\": 855852332034265088, \"lang\": \"en\", \"location\": \"Somewhere in the cloud\", \"name\": \"ArtistBot\", \"profile_background_color\": \"F5F8FA\", \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/855852332034265088/1492892354\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/855878764143804417/r55Z2Js5_normal.jpg\", \"profile_link_color\": \"1DA1F2\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"profile_text_color\": \"333333\", \"screen_name\": \"TheTalkativeBot\", \"status\": {\"created_at\": \"Tue Apr 25 22:31:31 +0000 2017\", \"id\": 856999137753145349, \"id_str\": \"856999137753145349\", \"in_reply_to_screen_name\": \"TheTalkativeBot\", \"in_reply_to_status_id\": 856894659007844352, \"in_reply_to_user_id\": 855852332034265088, \"lang\": \"en\", \"media\": [{\"display_url\": \"pic.twitter.com/lMV1EeegY0\", \"expanded_url\": \"https://twitter.com/TheTalkativeBot/status/856999137753145349/photo/1\", \"id\": 856999135043620864, \"media_url\": \"http://pbs.twimg.com/media/C-SsLDkXkAAvT-S.jpg\", \"media_url_https\": \"https://pbs.twimg.com/media/C-SsLDkXkAAvT-S.jpg\", \"sizes\": {\"large\": {\"h\": 1200, \"resize\": \"fit\", \"w\": 900}, \"medium\": {\"h\": 1200, \"resize\": \"fit\", \"w\": 900}, \"small\": {\"h\": 680, \"resize\": \"fit\", \"w\": 510}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"type\": \"photo\", \"url\": \"https://t.co/lMV1EeegY0\"}], \"source\": \"<a href=\\\"http://www.google.com\\\" rel=\\\"nofollow\\\">TheScenarioBot</a>\", \"text\": \"@dvp_tran Here what's your picture evokes to me ! https://t.co/lMV1EeegY0\"}, \"statuses_count\": 41}\n"
     ]
    }
   ],
   "source": [
    "print(api.VerifyCredentials())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "api.PostUpdate(\"Today I am learning German, watch me improve! ;) \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Learning from corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1. Load and convert data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Example script to generate text from Nietzsche's writings.\n",
    "At least 20 epochs are required before the generated text\n",
    "starts sounding coherent.\n",
    "It is recommended to run this script on GPU, as recurrent\n",
    "networks are quite computationally intensive.\n",
    "If you try this script on new data, make sure your corpus\n",
    "has at least ~100k characters. ~1M is better.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done concatenating file : 249.txt\n",
      "Done concatenating file : 4740-8.txt\n",
      "Done concatenating file : 799-0.txt\n",
      "Done concatenating file : 4548-8.txt\n",
      "Done concatenating file : 4791-8.txt\n",
      "Done concatenating file : 803-8.txt\n",
      "Done concatenating file : 2650-0.txt\n"
     ]
    }
   ],
   "source": [
    "#Load and concatenate files:\n",
    "\n",
    "DIR=\"../../LSTM/data/Gutenberg/ebooks-unzipped/French/\"\n",
    "all_files = [f for f in listdir(DIR) if isfile(join(DIR, f))]\n",
    "\n",
    "#choose how many files to concatenate:\n",
    "nb_files=7\n",
    "if nb_files>len(all_files):\n",
    "    nb_files=len(all_files)\n",
    "    \n",
    "    \n",
    "out_path=\"french/data/\"\n",
    "if not os.path.exists(out_path):\n",
    "    os.makedirs(out_path)\n",
    "if not os.path.exists(out_path+\"input/\"):\n",
    "    os.makedirs(out_path+\"input/\")\n",
    "    \n",
    "with open(out_path+'input/french.txt', 'w') as outfile:\n",
    "    for fname in all_files[0:nb_files]:\n",
    "        with open(DIR+fname) as infile:\n",
    "            i=0\n",
    "            for line in infile:\n",
    "                if i>=50:\n",
    "                    outfile.write(line)\n",
    "                i=i+1\n",
    "        print (\"Done concatenating file : %s\" %fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UTF-8'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ISO-8859-1\"\n",
    "\"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load file\n",
    "file_name=out_path+'input/french.txt'\n",
    "text = open(file_name).read()\n",
    "text=normalize('NFKD',text.decode('latin1')).encode('ASCII', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = re.sub(\"\\n\\n+\" , \"\\n\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 2508723\n",
      "total chars: 89\n"
     ]
    }
   ],
   "source": [
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "VOCAB_SIZE = len(chars)\n",
    "print('total chars:',VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** The RNN takes in input numerical data hence the necessity to convert strings into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating mapping between indexes and characters\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weâ€™re gonna use Keras to create and train our Network, so we must convert the data into this form: (number_of_sequences, length_of_sequence, number_of_features).\n",
    "- nb of features = length of the char array\n",
    "- length of sequence = batch size\n",
    "- nb of sequence = len(data) divided by batch size.\n",
    "\n",
    "**Warning : ** target sequence is setted by shifting the source/input sequence by one character with both having the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.82 s, sys: 196 ms, total: 2.02 s\n",
      "Wall time: 2.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "SEQ_LENGTH=100\n",
    "#Build three dimensional arrays\n",
    "X = np.zeros((len(text)/SEQ_LENGTH, SEQ_LENGTH, VOCAB_SIZE)) #input\n",
    "y = np.zeros((len(text)/SEQ_LENGTH, SEQ_LENGTH, VOCAB_SIZE)) #target\n",
    "\n",
    "#Build sequences\n",
    "for i in range(0, len(text)/SEQ_LENGTH):\n",
    "    X_sequence = text[i*SEQ_LENGTH:(i+1)*SEQ_LENGTH]\n",
    "    X_sequence_ix = [char_indices[value] for value in X_sequence]\n",
    "    input_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n",
    "    for j in range(SEQ_LENGTH):\n",
    "        input_sequence[j][X_sequence_ix[j]] = 1.\n",
    "    X[i] = input_sequence\n",
    "\n",
    "    y_sequence = text[i*SEQ_LENGTH+1:(i+1)*SEQ_LENGTH+1]\n",
    "    y_sequence_ix = [char_indices[value] for value in y_sequence]\n",
    "    target_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n",
    "    for j in range(SEQ_LENGTH):\n",
    "        target_sequence[j][y_sequence_ix[j]] = 1.\n",
    "    y[i] = target_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2. Build the network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HIDDEN_DIM= 500 #500\n",
    "LAYER_NUM = 2\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(LAYER_NUM - 1):\n",
    "    model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_text(model, length, vocab_size, ix_to_char):\n",
    "    # starting with random character\n",
    "    ix = [np.random.randint(vocab_size)]\n",
    "    y_char = [ix_to_char[ix[-1]]]\n",
    "    X = np.zeros((1, length, vocab_size))\n",
    "    for i in range(length):\n",
    "        # appending the last predicted character to sequence\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        print(ix_to_char[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(ix_to_char[ix[-1]])\n",
    "    return ('').join(y_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PZggglyyd_ddns89999955555555555555555555555555555555555555555555555555555555555555555555555555555555"
     ]
    }
   ],
   "source": [
    "# Generate some sample before training to know how bad it is!\n",
    "bla = generate_text(model, 100, VOCAB_SIZE, indices_char)\n",
    "#api.PostUpdate(status=bla[0:123])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Train network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_iternb(string):\n",
    "    return re.findall(r'checkpoint_500_epoch_(.*).hdf5', string)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at iteration : 800\n",
      "\n",
      "\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda2/lib/python2.7/site-packages/keras/models.py:834: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "92s - loss: 0.1989\n",
      "2.00\n",
      "_________________________\n",
      "\n",
      "\n",
      "Un soupir moribond\n",
      "trainait sur la place\n",
      "de l'espoir\n",
      "respirain, ils avaient lance les memes Ateints \n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "97s - loss: 0.1869\n",
      "Mais Joe, Samuel?\n",
      "\n",
      "--Je ne l'abandonne pas! non certes! et dut l'ouragan s'enfoncer toute la partie de la\n",
      "construction en la tablier!  je\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "94s - loss: 0.1822\n",
      "] et\n",
      "neuf cent soixante-seize fois moins vite que dans cette atmosphA re place A \n",
      "une vie de mon amie avec collA gues, et enfin aux belles\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "93s - loss: 0.1836\n",
      "%sul.\n",
      "\n",
      "L'orbre fut des morts, mais des siennes paraissait chasser un des\n",
      "foyers, et qu'il n'eut pas le seul a faire aux deux angegues de\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "94s - loss: 0.1849\n",
      "Il ne manquait plus que cela! Voila donc ce\n",
      "quartzez-li.  Nous nous demandons, devant ce souvenir isolAme\n",
      "oA1 les palisites on garaissaien\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "94s - loss: 0.1855\n",
      "Je ne crois pas, dit-il, que l'on puisse parvenir a l'arrivee du pays de\n",
      "l'Islande.\n",
      "\n",
      "J'apercus un petit battage, qui venait de l'histoire\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "92s - loss: 0.1847\n",
      "\\(); beano of any work and other work\n",
      "restributing Project Gutenberg-tm electronic works to\n",
      "protect the PROJECT GUTENBERG-tm concept and t\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "92s - loss: 0.1851\n",
      "Je le crois; d'ailleurs, au besoin, on mange curieux d'une parille\n",
      "ideex, qui calculait les moyens de ne point a mon\n",
      "corps; un Insecie cru\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "92s - loss: 0.1840\n",
      "Odette s'Atait refroidie A  son\n",
      "Agard, ce n'est qu'en mettant en regard de ce qu'elle Atait\n",
      "aujourd'hui aux barbes, et, A  quelques milles\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "91s - loss: 0.1846\n",
      "The Project Gutenberg Literary Archive Foundation\n",
      "\n",
      "The Project Gutenberg Literary Archive Foundation and Michael\n",
      "Hart, the owner of the P\n",
      "\n",
      "Iteration nb : 810\n",
      "\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "91s - loss: 0.1839\n",
      ", construit par la logique d'un crAateur et on venait de\n",
      "s'absenter, devait Aatre dans l'Atat de ma tenue. Non; Als grossiA res Ataient rAs\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "91s - loss: 0.1845\n",
      "0 C5_C1_   B3 A2 D0 C5 B4 CF       E6 E6 C1 DA_BC_D0\n",
      "   _D0_CF A2 D0 D0 E6        . B3 BC B4 E6 B4       C5 C5 A2 CF A2 DA       A0 E6 D0 C\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "91s - loss: 0.1843\n",
      "Franquetot, lesquelles, parce qu'elles Ataient\n",
      "cousines, passaient leur temps qui le suivait, mais ce fut un\n",
      "niveau soigneur de seize cinq\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "91s - loss: 0.1843\n",
      ".  Le rA le jouA dans cette rencontre par le chevaleresque\n",
      "Atait comme le sienne, parce qu'il l'y fait, pourtant\n",
      "l'imagination avec ces no\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "91s - loss: 0.1834\n",
      "t entiere entre mes levres.\n",
      "\n",
      "Oh!  jouissance de dix mines pas disparurent les iles ne subiteras pas\n",
      "abandon et la fente avec des divers s\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "91s - loss: 0.1830\n",
      "9\n",
      "\n",
      "--Assez donc, je vous regarderai de me resoudre sans trop dur jour\n",
      "un detaille encore reste sur le spectacle tombeau.\n",
      "\n",
      "Retournons, qu\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "91s - loss: 0.1833\n",
      ".  Le lendemain, le fait generait un peu plus\n",
      "tard de la tete de dospose.\n",
      "Les calculs du professeur etaient rares.  Le ciel devenait une e\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "91s - loss: 0.1852\n",
      "(any work on which the\n",
      "phrase \"Project Gutenberg\" appears, or with which the phrase \"Project\n",
      "Gutenberg\" is associated) is accessed, displa\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "91s - loss: 0.1840\n",
      "A  la main, et, la mine aidant, nous\n",
      "irastions.\n",
      "\n",
      "--Ainsi, dans cette contree charmante de votre petit morceau de\n",
      "Joe le bara; je ne vis \n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "91s - loss: 0.1845\n",
      "!  mon oncle, ce morceau de viande dechique icorentr. Il ne put\n",
      "se mouver? repondit le prAsident.\n",
      "\n",
      "--Et mon cher de dieux nous elevez rep\n",
      "\n",
      "Iteration nb : 820\n",
      "\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "91s - loss: 0.1840\n",
      "@ptible,\n",
      "des parois de deux bouquets et toutes chacunes du\n",
      "Shari; Contre le ciel batta comme s'ils velles du Figur on\n",
      "reconnut le _Tampa-\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "91s - loss: 0.1838\n",
      "________________________\n",
      "\n",
      "\n",
      "Bel espoir.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "XXVII\n",
      "\n",
      "\n",
      "Je n'aurais mieux.  C'etait a n'yergis inquiet, ces milles dans le bois de\n",
      "co\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "91s - loss: 0.1842\n",
      "/copyright) agreement, you must obtain permission in writing from\n",
      "both the Project Gutenberg Literary Archive Foundation (\"the Foundation\"\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "95s - loss: 0.1819\n",
      "\u0019 Project Gutenberg-tm\n",
      "electronic work, you indicate that you have read, understand, agree to\n",
      "any fee, or can a     http://www.gutenberg.o\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "103s - loss: 0.1833\n",
      "Verdurin qui ne\n",
      "s'effrayaient pas qu'une femme eAt un annAe oA1 la lutte au fond de cette monotonisiere), un peu de ce qui\n",
      "remplie d'invar\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "110s - loss: 0.1831\n",
      "RRANTY OR BREACH OF CONTRACT EXCEPT THOSE\n",
      "PROVIDED IN PARAGRAPH 1.F.3.  YOU AGREE THAT THE FOUNDATION, THE\n",
      "TRADEMARK OWNER, AND ANY DISTRI\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "#batch size equals to seq length here\n",
    "BATCH_SIZE=100\n",
    "#len of desired output\n",
    "GENERATE_LENGTH=140\n",
    "DIR=out_path+\"weights/weight_attempt_s03/\"\n",
    "flag=True\n",
    "\n",
    "try:\n",
    "    onlyfiles = [f for f in listdir(DIR) if isfile(join(DIR, f))]\n",
    "    iteration=[]\n",
    "    for files in onlyfiles:\n",
    "        iteration.append(int(get_iternb(files)))\n",
    "    iteration=max(iteration)\n",
    "\n",
    "    last_checkpoint=DIR+onlyfiles[0][0:21]+str(iteration)+'.hdf5'\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    onlyfiles=[]\n",
    "    if not os.path.exists(DIR):\n",
    "        os.makedirs(DIR)\n",
    "    nb_files=0\n",
    "\n",
    "if nb_files>0:\n",
    "    model.load_weights(last_checkpoint)\n",
    "else:\n",
    "    iteration=0\n",
    "    \n",
    "print(\"Starting at iteration : %s\" %iteration)\n",
    "while flag==True:\n",
    "    print('\\n')\n",
    "    print('-'*20)\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, verbose=2, nb_epoch=1)\n",
    "    iteration += 1\n",
    "    bla=generate_text(model, GENERATE_LENGTH,VOCAB_SIZE, indices_char)\n",
    "    if iteration % 10 == 0:\n",
    "        print(\"\\n\\nIteration nb : %s\" %iteration)\n",
    "        #api.PostUpdate(status=bla[0:123])\n",
    "        model.save_weights(DIR+'checkpoint_{}_epoch_{}.hdf5'.format(HIDDEN_DIM, iteration))\n",
    "        #remove unecessary files:\n",
    "        for files in onlyfiles:\n",
    "            try:\n",
    "                if files:\n",
    "                    os.remove(DIR+files)\n",
    "            except:\n",
    "                pass\n",
    "        onlyfiles = [f for f in listdir(DIR) if isfile(join(DIR, f))]\n",
    "\n",
    "    if iteration>=900:\n",
    "        print(\"Stopping...\")\n",
    "        flag=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Generate text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_text(model, length, vocab_size, ix_to_char):\n",
    "    # starting with random character\n",
    "    ix = [np.random.randint(vocab_size)]\n",
    "    y_char = [ix_to_char[ix[-1]]]\n",
    "    X = np.zeros((1, length, vocab_size))\n",
    "    for i in range(length):\n",
    "        # appending the last predicted character to sequence\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        print(ix_to_char[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(ix_to_char[ix[-1]])\n",
    "    if not os.path.exists(out_path+\"generate/\"):\n",
    "        os.makedirs(out_path+\"generate/\")\n",
    "    with open(out_path+\"generate/output.txt\",\"w\") as f:\n",
    "        f.write(('').join(y_char))\n",
    "    return ('').join(y_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#seed with particular text:\n",
    "def generate_text_seeded(model,seed,length, vocab_size, ix_to_char):\n",
    "    # starting with random character\n",
    "    # char_indices\n",
    "    ix = [char_indices[x] for x in seed]\n",
    "    y_char = [x for x in seed]\n",
    "    X = np.zeros((1, length, vocab_size))\n",
    "    for i in range(len(ix)) :\n",
    "        X[0, i, :][ix[i]] = 1\n",
    "        print(ix_to_char[ix[i]], end=\"\")\n",
    "    to_substract = len(ix)\n",
    "    for i in range(length-to_substract):\n",
    "        # appending the last predicted character to sequence\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        print(ix_to_char[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(ix_to_char[ix[-1]])\n",
    "    return ('').join(y_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_text_seeded(model,normalize('NFKD',\"le roi est mort \".decode('latin1')), 1000, VOCAB_SIZE, indices_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "out = save_text(model, 1500, VOCAB_SIZE, indices_char)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
