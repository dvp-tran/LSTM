{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Dropout,TimeDistributed\n",
    "from keras.layers import LSTM,SimpleRNN\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os, os.path\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from unicodedata import normalize\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- ### Check GPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'/gpu:0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Check and set Twitter's API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "api = twitter.Api(consumer_key='LyNVanTEQEOEGKfXAMeLv6AKG',\n",
    "                    consumer_secret='0lJvhaaOP5cRZWm6rxwyBIAypd1P7eiDx9f74KBDlLrSldNuBQ',\n",
    "                    access_token_key='855852332034265088-geTEVmA7xIsOD3WCZyfBNnqjRdS1MhW',\n",
    "                    access_token_secret='kJMwMl67e3nYrqaGWzIizxzQpRZhtBfOnwPflO1fk3cOt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"created_at\": \"Sat Apr 22 18:34:31 +0000 2017\", \"default_profile\": true, \"description\": \"Learning how to be creative\", \"followers_count\": 2, \"friends_count\": 3, \"id\": 855852332034265088, \"lang\": \"en\", \"location\": \"Somewhere in the cloud\", \"name\": \"ArtistBot\", \"profile_background_color\": \"F5F8FA\", \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/855852332034265088/1492892354\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/855878764143804417/r55Z2Js5_normal.jpg\", \"profile_link_color\": \"1DA1F2\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"profile_text_color\": \"333333\", \"screen_name\": \"TheTalkativeBot\", \"status\": {\"created_at\": \"Mon Apr 24 13:46:57 +0000 2017\", \"id\": 856504737352601601, \"id_str\": \"856504737352601601\", \"in_reply_to_screen_name\": \"dvp_tran\", \"in_reply_to_status_id\": 856504560373911552, \"in_reply_to_user_id\": 747074580754403328, \"lang\": \"en\", \"source\": \"<a href=\\\"http://www.google.com\\\" rel=\\\"nofollow\\\">TheScenarioBot</a>\", \"text\": \"@dvp_tran Automate answer !\"}, \"statuses_count\": 49}\n"
     ]
    }
   ],
   "source": [
    "print(api.VerifyCredentials())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "api.PostUpdate(\"Today I am learning German, watch me improve! ;) \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Learning from corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1. Load and convert data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Example script to generate text from Nietzsche's writings.\n",
    "At least 20 epochs are required before the generated text\n",
    "starts sounding coherent.\n",
    "It is recommended to run this script on GPU, as recurrent\n",
    "networks are quite computationally intensive.\n",
    "If you try this script on new data, make sure your corpus\n",
    "has at least ~100k characters. ~1M is better.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done concatenating file : poemes1.txt\n"
     ]
    }
   ],
   "source": [
    "#Load and concatenate files:\n",
    "\n",
    "DIR=\"../../LSTM/data/poemes/\"\n",
    "all_files = [f for f in listdir(DIR) if isfile(join(DIR, f))]\n",
    "\n",
    "#choose how many files to concatenate:\n",
    "nb_files=7\n",
    "if nb_files>len(all_files):\n",
    "    nb_files=len(all_files)\n",
    "    \n",
    "    \n",
    "out_path=\"poetry/data/\"\n",
    "if not os.path.exists(out_path):\n",
    "    os.makedirs(out_path)\n",
    "if not os.path.exists(out_path+\"input/\"):\n",
    "    os.makedirs(out_path+\"input/\")\n",
    "    \n",
    "with open(out_path+'input/poemes.txt', 'w') as outfile:\n",
    "    for fname in all_files[0:nb_files]:\n",
    "        with open(DIR+fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)\n",
    "        print (\"Done concatenating file : %s\" %fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load file\n",
    "file_name=out_path+'input/poemes.txt'\n",
    "text = open(file_name).read()\n",
    "text=normalize('NFKD',text.decode('latin1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "to_delete = u'Skip to main content\\nweb\\ntexts\\nmovies\\naudio\\nsoftware\\nimage\\nlogo\\nToggle navigation\\nsearch\\nSearch the Archive\\nSearch\\n Search\\n\\nupload\\npersonSIGN IN\\nFull text of \"Quelques poA\\u0303 \\u0308mes\"\\nSee other formats\\nThis is a digital copy of a book that was preserved for gA\\u0303\\xa9nA\\u0303\\xa9rations on library shelves before it was carefully scanned by Google as part of a project \\nto make the world\\'s books discoverable online. \\n\\nIt bas survived long enough for the copyright to expire and the book to enter the public domain. A public domain book is one that was never subject \\nto copyright or whose lA\\u0303\\xa9gal copyright term has expired. Whether a book is in the public domain may vary country to country. Public domain books \\nare our gateways to the past, representing a wealth of history, culture and knowledge that \\'s often difficult to discover. \\n\\nMarks, notations and other marginalia prA\\u0303\\xa9sent in the original volume will appear in this file - a reminder of this book\\' s long journey from the \\npublisher to a library and finally to y ou. \\n\\nUsage guidelines \\n\\nGoogle is proud to partner with libraries to digitize public domain materials and make them widely accessible. Public domain books belong to the \\npublic and we are merely their custodians. Nevertheless, this work is expensive, so in order to keep providing this resource, we hA\\u0303\\xa2ve taken steps to \\nprevent abuse by commercial parties, including placing technical restrictions on automated querying. \\n\\nWe also ask that y ou: \\n\\n+ Make non-commercial use of the files We designed Google Book Search for use by individuals, and we request that you use thA\\u0303 \\u0308se files for \\nPersonal, non-commercial purposes. \\n\\n+ Refrain from automated querying Do not send automated queries of any sort to Google\\'s System: If you are conducting research on machine \\ntranslation, optical character rA\\u0303\\xa9cognition or other areas where access to a large amount of text is helpful, please contact us. We encourage the \\nuse of public domain materials for thA\\u0303 \\u0308se purposes and may be able to help. \\n\\n+ Maintain attribution The Google \"watermark\" you see on each file is essential for informing people about this project and helping them find \\nadditional materials through Google Book Search. Please do not remove it. \\n\\n+ Keep it lA\\u0303\\xa9gal Whatever your use, remember that you are responsible for ensuring that what you are doing is lA\\u0303\\xa9gal. Do not assume that just \\nbecause we believe a book is in the public domain for users in the United States, that the work is also in the public domain for users in other \\ncountries. Whether a book is still in copyright varies from country to country, and we can\\'t offer guidance on whether any spA\\u0303\\xa9cifie use of \\nany spA\\u0303\\xa9cifie book is allowed. Please do not assume that a book\\'s appearance in Google Book Search means it can be used in any manner \\nany where in the world. Copyright infringement liability can be quite severe. \\n\\nAbout Google Book Search \\n\\nGoogle\\'s mission is to organize the world\\'s information and to make it universally accessible and useful. Google Book Search helps readers \\ndiscover the world\\'s books while helping authors and publishers reach new audiences. You can search through the full text of this book on the web \\n\\nat http : //books . google . com/| \\n\\n\\n\\nDigitized by \\n\\n\\n\\nGoogle \\n\\n\\n\\nDigitized by \\n\\n\\n\\nGoogle \\n\\n\\n\\nDigitized by \\n\\n\\n\\nGoogle \\n\\n\\n\\nDigitized by \\n\\n\\n\\nGoogle \\n\\n\\n\\n\\n\\n\\nDigitized by \\n\\n\\n\\nGoogle \\n\\n\\n\\nDigitized by \\n\\n\\n\\nGoogle \\n\\n\\n\\nDigitized by \\n\\n\\n\\nGoogle \\n\\n\\n\\nDigitized by \\n\\n\\n\\nGoogle \\n\\n\\n\\nDigitized by \\n\\n\\n\\nGoogle \\n\\n\\n\\nDigitized by \\n\\n\\n\\nGoogle \\n\\n\\n\\n/>A\\u0302\\xab, \\n\\n\\n\\nQUELQUES POA\\u0303\\x8bMBS \\n\\n\\n\\nDigitized by \\n\\n\\n\\nGoogle \\n\\n\\n\\nIL A A\\u0303 tA\\u0303\\xa9 TinA\\u0303\\xa9 DE CET OUVBAGB : \\n\\nTreize exemplaires Japon impA\\u0303\\xa9rial, \\n\\n{dont trois hors commerce) numA\\u0303\\xa9rotA\\u0303\\xa9s \\n\\nde 1 A\\u0303 iO et de il A\\u0303  iS \\n\\nVingt exemplaires vA\\u0303\\xa9lin de Rives \\n\\n{dont cinq hors commerce) numA\\u0303\\xa9rotA\\u0303\\xa9s \\n\\nde U A\\u0303  28 et de 29 k 33 \\n\\nHuit cent quatre-vingts exemplaires ordinaires \\n\\n(dont quatre-vingts hors commerce) numA\\u0303\\xa9rotA\\u0303\\xa9s \\n\\nde 34 A\\u0303  833 et de 834 A\\u0303  9i3, \\n\\n\\n\\nNA\\u0302\\xbb221 \\n\\n\\n\\nCopyright by G. CrA\\u0303 \\u0308s et C\", 19f6 \\nTous droits de reproduction et d\\'adaptation rA\\u0303\\xa9servA\\u0303\\xa9s pour tous pays. \\n\\n\\n\\nDigitized by \\n\\n\\n\\nGoogle \\n\\n\\n\\nDigitized by \\n\\n\\n\\nGoogle \\n\\n\\n\\nDigitized by \\n\\n\\n\\nGoogle \\n\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = text.replace(to_delete,\"\").replace('Digitized by',\"\").replace('Google',\"\") \n",
    "text = re.sub(\"\\n\\n+\" , \"\\n\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 72089\n",
      "total chars: 108\n"
     ]
    }
   ],
   "source": [
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "VOCAB_SIZE = len(chars)\n",
    "print('total chars:',VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** The RNN takes in input numerical data hence the necessity to convert strings into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating mapping between indexes and characters\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re gonna use Keras to create and train our Network, so we must convert the data into this form: (number_of_sequences, length_of_sequence, number_of_features).\n",
    "- nb of features = length of the char array\n",
    "- length of sequence = batch size\n",
    "- nb of sequence = len(data) divided by batch size.\n",
    "\n",
    "**Warning : ** target sequence is setted by shifting the source/input sequence by one character with both having the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 64 ms, sys: 20 ms, total: 84 ms\n",
      "Wall time: 83 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "SEQ_LENGTH=100\n",
    "#Build three dimensional arrays\n",
    "X = np.zeros((len(text)/SEQ_LENGTH, SEQ_LENGTH, VOCAB_SIZE)) #input\n",
    "y = np.zeros((len(text)/SEQ_LENGTH, SEQ_LENGTH, VOCAB_SIZE)) #target\n",
    "\n",
    "#Build sequences\n",
    "for i in range(0, len(text)/SEQ_LENGTH):\n",
    "    X_sequence = text[i*SEQ_LENGTH:(i+1)*SEQ_LENGTH]\n",
    "    X_sequence_ix = [char_indices[value] for value in X_sequence]\n",
    "    input_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n",
    "    for j in range(SEQ_LENGTH):\n",
    "        input_sequence[j][X_sequence_ix[j]] = 1.\n",
    "    X[i] = input_sequence\n",
    "\n",
    "    y_sequence = text[i*SEQ_LENGTH+1:(i+1)*SEQ_LENGTH+1]\n",
    "    y_sequence_ix = [char_indices[value] for value in y_sequence]\n",
    "    target_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n",
    "    for j in range(SEQ_LENGTH):\n",
    "        target_sequence[j][y_sequence_ix[j]] = 1.\n",
    "    y[i] = target_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2. Build the network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HIDDEN_DIM= 500 #500\n",
    "LAYER_NUM = 2\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(LAYER_NUM - 1):\n",
    "    model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_text(model, length, vocab_size, ix_to_char):\n",
    "    # starting with random character\n",
    "    ix = [np.random.randint(vocab_size)]\n",
    "    y_char = [ix_to_char[ix[-1]]]\n",
    "    X = np.zeros((1, length, vocab_size))\n",
    "    for i in range(length):\n",
    "        # appending the last predicted character to sequence\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        print(ix_to_char[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(ix_to_char[ix[-1]])\n",
    "    return ('').join(y_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "«bb)))&B&&&33gggiQ33ggQQ333gg3gQi33ggdddd66E222224444«««««««bbbb33333ggii3ggdddd::4444P4xPPX«««««««"
     ]
    }
   ],
   "source": [
    "# Generate some sample before training to know how bad it is!\n",
    "bla = generate_text(model, 100, VOCAB_SIZE, indices_char)\n",
    "#api.PostUpdate(status=bla[0:123])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Train network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_iternb(string):\n",
    "    return re.findall(r'checkpoint_500_epoch_(.*).hdf5', string)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at iteration : 450\n",
      "\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0435\n",
      "RÃMES \n",
      "Entre de morts cratÃ ̈res toujours lumineux, â \n",
      "Eternellement calmes, non touchÃ©s de la Jes. \n",
      " \n",
      " \n",
      "142 QUELQUES POÃMES \n",
      "Et a\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0410\n",
      "T. -A. Hoffmann. â Le Chat-Mourre. \n",
      "Charles V.vn Lerberghb. â Les Flaireurs. â PoÃ ̈mes. \n",
      "Mais TÃ©nÃ ̈res et pors es nous loin I\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0432\n",
      "̄e sang est les \n",
      "PÃ©missement l'ordide son pas \n",
      "d'ait qu'in couleat, et qui antÃ© d'une jeune des \n",
      "tuits veuxtus Ã  peine \n",
      "Comme le mysui\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0478\n",
      "« la pensÃ©e puissante 1 . . \n",
      "Non I le ciel n'est pas lÃ , \n",
      "Dans le profond Espace hantÃ© de cauche, un diber. \n",
      "Sous seraient une est pas\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0416\n",
      "Ã©tait l'ennemi, il en \n",
      "est deux fois plus, \n",
      "Et qui s'approchent, et muets^ et menaÃ§ants. \n",
      "Et la prisÃ©e de lous solident la rÃ©aient l\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0407\n",
      "̃©es, \n",
      "Et la mÃ©moire est vivante. \n",
      "Et sonores sont les mots. \n",
      "Les jours s'en vont, â \n",
      "Mais il es  st vers \n",
      "hants de Tabentre et les cau\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0446\n",
      "3 \n",
      "Parmi les SorciÃ ̈res qui frissonnaient des caresses de \n",
      "Toi. \n",
      "On nous tourmentait parce que nous ampÃ©son ! \n",
      "Et le mÃame se travaglem\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0419\n",
      "ue les attire. \n",
      "Elles veulent Tamour, les rayons, la tempÃate. \n",
      "Elles rÃavent de fleurs qui seraient s'en ellence. \n",
      "El est ce qui nous l'e\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0426\n",
      "x des hautes Ãcoles. Ces a dÃ©- \n",
      "sordres Â» ou Â« Ã©meutes Â» avaient pour point de dÃ©part, \n",
      "J'Ã©tais Ã©tainna vague qui partemps\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0416\n",
      "©toilÃ©e. \n",
      "De Tarc-en-ciel je ferai une route, \n",
      "Au-dessus de Tabime plein de tonnerres \n",
      "J'Ã©rigeraitse, enpre de Tus le Sienges \n",
      "D'ailairs\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0413\n",
      "®tre \n",
      "leur littÃ©rature, ne gardant pas pour lui-mÃame, d^ail- \n",
      " \n",
      " \n",
      "PRÃFACB 15 \n",
      "leurs, ces dÃ©lisse : \n",
      "Le pouppin de son Ã©tait pielle\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0422\n",
      "¢me 1 \n",
      " \n",
      " \n",
      "LES NAVIRES MORTS 37 \n",
      "LES NAVIRES MORTS \n",
      "Pris en remuements dans les glaces, dans Taccalmi inconnable de nuis regardes corches se\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0425\n",
      "ES \n",
      "Comme le Soleil, jeune toujours^ \n",
      "Caressons les fleurs^ les fleurs qui flamboient, \n",
      "L'air transparence les coursÃ©es. \n",
      " \n",
      " \n",
      "PRÃE CE Le\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0447\n",
      "« de temps Ã  toute partit en mÃ ̈re une rÃ©dintement ! \n",
      " \n",
      " \n",
      "LE POÃFE \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "LONNONSINN IMMIN B \n",
      "PORSI NE PRAIVE \n",
      "Rague et gr\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0438\n",
      "s des Montagnes (1904). \n",
      "Les Eparres blancs (1908). \n",
      "Fleurs DE Serpent. â Lettres de route eux blÃ©lest. \n",
      "Et elle soutace de morteur. \n",
      "L\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0413\n",
      "3 \n",
      "Parmi les SorciÃ ̈res qui frissonnaient des caresses de \n",
      "Toi. \n",
      "On nous tourmentait parce que nous ampÃ©sonne, maisient un  littÃ©ratur\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0413\n",
      "s la Jeunesse n'est qu'Ã©cho \n",
      "D'un rire lointain, \n",
      "N*est que rÃ©sonnance de pas qui s'Ã©loignent ! \n",
      " \n",
      " \n",
      "1 \n",
      "POÃFACE 13 \n",
      "soir qui alors c\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0411\n",
      "BeautÃ©. \n",
      " \n",
      " \n",
      "LES ACCORDS \n",
      "21) \n",
      "LES ACCORDS \n",
      "En la beautÃ© des musiques^ \n",
      "Ainsi qu'en TimmutabilitÃ© dus conders, \n",
      "La poril en le toute, \n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0413\n",
      "t le long des pentes... \n",
      " \n",
      " \n",
      "LA TERRE \n",
      "10Â» \n",
      "LA TERRE \n",
      "La Terre enseigne Ã  regarder profondÃ©ment et vois ! \n",
      "L'Ã©toute de plus doux s'a\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0419\n",
      "5 Appels de l'anfiquite (Traductions de poÃ ̈mes sa- \n",
      "crÃ©s des peuples de TAntiquitÃ©, d'Orient es dÃ©s littÃ©raires, phison Tommeil e\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0427\n",
      "Nuit sans larmes, toute Ã©toilÃ©e s'approche. â \n",
      "Regarde 1 \n",
      " \n",
      " \n",
      "l'instable 147 \n",
      "L'INSTABLE \n",
      "Dans le print de transillablement des coupr\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0423\n",
      "Gymnase de toutes mes forces Â», con- \n",
      "tinue le poÃ ̈te, de qui le systÃ ̈me nerveux demeura \n",
      " \n",
      " \n",
      " \n",
      "LES COARMES DE SA EL \n",
      "Â« JAUE \n",
      "En ne\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0430\n",
      "ur mortelle. \n",
      "Le royaume de la Mort blanche n'a, nulle part, de limites. . . \n",
      "â Et,qu'Ãates-vous une brandis les Saloiss^ \n",
      "Qu'is peut en\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0428\n",
      "urs : \n",
      "Tais ju n'aubleur de Fartine. â \n",
      "Le rÃave des jours demeurait en elle, comme la \n",
      "glace qui dort sans fondre. \n",
      "Et elle Ã©tait, t\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0421\n",
      "_ \n",
      " \n",
      " \n",
      "SOYONS COMME LE SOLEIL 27 \n",
      "SOYONS COMME LE SOLEIL \n",
      "Soyons comme le Soleil ! Oublions \n",
      "Qui nous Ãatrincel en lorme de feu. \n",
      "Et les pa\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0452\n",
      "¢ . . , 43 \n",
      "La Ville 45 \n",
      "Le poisson dorÃ© 49 \n",
      "11 \n",
      " \n",
      " \n",
      "162 TABLE \n",
      "Comme un Espagnol. '53 \n",
      "Doucement farmidiques de trÃ ̈s : \n",
      "le tÃ©nÃ ̈b\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0424\n",
      ") de basailles et de carais que la littÃ©rature puesde pa les prÃ©les, \n",
      "Et des rosant remaudests, se toujes, â comme l'extrefille de li\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "3s - loss: 0.0454\n",
      "̂ \n",
      "Les musiciens chantaient. \n",
      "Une brise, au jardin, balanÃ§ait \n",
      "Une balanÃ§oire lÃ©gÃ ̈re. \n",
      "Au vois ! elle queur sous autre au se past\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "3s - loss: 0.0412\n",
      "4) \n",
      "Les nuits et les jours, \n",
      "Les tÃ©nÃ ̈bres et lÃ©s feux. \n",
      "Brillent les pensÃ©es. \n",
      "Et la mÃ©moire est vivante. \n",
      "Et sonores sont les mo\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0398\n",
      "me Ã©tendue, en seul miroir. \n",
      "â â¬ Cher ! Ã ́ mon cher ! Je t'aime ! Â».â¢. \n",
      "Minuit, de Thampe ! \n",
      "I nele fomiÃ ̈re : â Hissit\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0400\n",
      "ys c&line Tonde. \n",
      " \n",
      " \n",
      "m QUELQUES POÃMES \n",
      "Mon ouÃ ̄e, sans que j'Ã©coute, saisit \n",
      "Que balbutie Taleure, la ser- \n",
      "somess Ã  la poÃ ̈te l\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0414\n",
      "1 \n",
      "SUR L'EAU \n",
      "De la nef y glissa la rame. \n",
      "Caressante, pÃ¢me la fratcheur. \n",
      "â Â« Cher, Ã ́ mon s'ent taut jeux. \n",
      "CrÃ©es de l'Ã¢me, \n",
      "\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0418\n",
      "́  la prÃ©cision du FranÃ§ais, un \n",
      "certain vague destinÃ© Ã  suggÃ©rer plus qu'Ã  dire... \n",
      "Notre la leurs coutrauxion du relue, \n",
      "Une b\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0441\n",
      "»re prÃ©occupation a Ã©tÃ© d'Ãatre absolument fidÃ ̈les Ã  \n",
      "la pensÃ©e du poÃ ̈te, Ã  son sent les poillÃ©s I \n",
      "Qui ne soutiÃ ̈re \n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0412\n",
      "HYY le montatre des Fue- \n",
      "benne, et despÃ©ritser : \n",
      "Je mers Ã  parre- avait des crÃ©aÃ©rabent s'approÃ ̈te fond la \n",
      "la value compe \n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0409\n",
      "»re \n",
      "derri en comme le Soleil ! \n",
      "Allois au loin une rour Ã©tuit la lumiÃ©e. \n",
      "Comme de l'ancoure du ait coure, \n",
      "Et me dont le profondeur de\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0416\n",
      "ys c&line Tonde. \n",
      " \n",
      " \n",
      "m QUELQUES POÃMES \n",
      "Mon ouÃ ̄e, sans que j'Ã©coute, saisit \n",
      "Que balbutie Taleurs que Terrent pas les paremis de la \n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0425\n",
      "fumÃ©e s'en Ã©chappait, qui de nouveau \n",
      "Ã©tait en elles, â \n",
      "Et son Ã©cho transposait en mÃ ̈te en \n",
      "hauts dis la liserre et le mÃame\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0417\n",
      "ES \n",
      "Comme le Soleil, jeune toujours, â \n",
      "Comme un bÃ©tail ! mi sondÃ ̈rent parmi Touts frÃ©lles prumesse et porte, sans pas des faderre\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0476\n",
      "x des Montagnes (1904). \n",
      "Les Eparres blancs (1908). \n",
      "Fleurs DE Serpent. â Lettres de route du Mexilier â \n",
      "Avous in Tins, et au jeuns d\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0396\n",
      "̂ \n",
      "Les musiciens chantaient. \n",
      "Une brise, au jardin, balanÃ§ait \n",
      "Une balanÃ§oire lÃ©gÃ ̈re. \n",
      "Au son heptit de soient l'enhement ! \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0401\n",
      "(1907). \n",
      "Les Oiseaux dans l'air (1908). \n",
      "Le vert Verger (1909). \n",
      "La ronde des Temps (1909). \n",
      "Le Reflend (1908). \n",
      "Le romands nocsinqussement \n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0409\n",
      "TAIT \n",
      "Le Pays qui se tait, tout en blanc, tout blanc, â \n",
      "Comme une FiancÃ©e couverte du voile \n",
      "Qui les tendre sonopes regardent. \n",
      "Some d\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "3s - loss: 0.0414\n",
      "/ \n",
      "Et blanc, â \n",
      "Comme une rÃ©chanf des poÃ ̈tes de la pÃ©nouie d'uneau \n",
      "Au d'ait chantÃ©, \n",
      "Et pour le prÃ©cice ralation de tout Ã  \n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0410\n",
      "vallÃ©es, \n",
      "Et celaient dans la ruche, avec le miel, la cire \n",
      "dÃ©corÃ©e. \n",
      " \n",
      " \n",
      "142 QUELQUES POÃMES \n",
      "Le lointain des steppes \n",
      "Un gointe pr\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0417\n",
      "gant FÃate, que TourguenelT \n",
      "qualifiait de Â« poÃ ̈te des poÃ ̈tes Â«.C'est le seul NÃ©cras- \n",
      " \n",
      "dÃ©sique est entendant parment les mo\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "3s - loss: 0.0429\n",
      "[ \n",
      "Et tout l'abbet souderant de pÃ ́te escerdes ou pappessÃ©es de la profondeur des eaux rÃ©palÃ©es. \n",
      "Et l'Ã©tait de la sel divenue. \n",
      "S\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0443\n",
      "TAIT \n",
      "Le Pays qui se tait, tout en blanc, tout blanc, â \n",
      "Comme une FiancÃ©e couverte du voile \n",
      "Qui le tentÃ©endent parme de sourgres, \n",
      "\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0422\n",
      "2¢ \n",
      " \n",
      " \n",
      "SOYONS COMME LE SOLEIL 27 \n",
      "SOYONS COMME LE SOLEIL \n",
      "Soyons comme le Soleil ! Oublions \n",
      "Qui nous nous pas la \n",
      "un une ourne pas une prA\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "2s - loss: 0.0428\n",
      ") de la beautÃ©, des Åuvres d'art. Mais la direction gÃ©nÃ©- \n",
      "rale persiste, et s'accroÃ®t Ã  sa son viglait. \n",
      "Et les routa na sans r\n",
      "\n",
      "Iteration nb : 500\n",
      "Stopping...\n"
     ]
    }
   ],
   "source": [
    "#batch size equals to seq length here\n",
    "BATCH_SIZE=100\n",
    "#len of desired output\n",
    "GENERATE_LENGTH=140\n",
    "DIR=out_path+\"weights/weight_attempt_s01/\"\n",
    "flag=True\n",
    "\n",
    "try:\n",
    "    onlyfiles = [f for f in listdir(DIR) if isfile(join(DIR, f))]\n",
    "    iteration=[]\n",
    "    for files in onlyfiles:\n",
    "        iteration.append(int(get_iternb(files)))\n",
    "    iteration=max(iteration)\n",
    "\n",
    "    last_checkpoint=DIR+onlyfiles[0][0:21]+str(iteration)+'.hdf5'\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    if not os.path.exists(DIR):\n",
    "        os.makedirs(DIR)\n",
    "    nb_files=0\n",
    "\n",
    "if nb_files>0:\n",
    "    model.load_weights(last_checkpoint)\n",
    "else:\n",
    "    iteration=0\n",
    "    \n",
    "print(\"Starting at iteration : %s\" %iteration)\n",
    "while flag==True:\n",
    "    print('\\n')\n",
    "    print('-'*20)\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, verbose=2, nb_epoch=1)\n",
    "    iteration += 1\n",
    "    bla=generate_text(model, GENERATE_LENGTH,VOCAB_SIZE, indices_char)\n",
    "    if iteration % 50 == 0:\n",
    "        print(\"\\n\\nIteration nb : %s\" %iteration)\n",
    "        #api.PostUpdate(status=bla[0:123])\n",
    "        model.save_weights(DIR+'checkpoint_{}_epoch_{}.hdf5'.format(HIDDEN_DIM, iteration))\n",
    "        #remove unecessary files:\n",
    "        for files in onlyfiles:\n",
    "            os.remove(DIR+files)\n",
    "        onlyfiles = [f for f in listdir(DIR) if isfile(join(DIR, f))]\n",
    "\n",
    "    if iteration>=500:\n",
    "        print(\"Stopping...\")\n",
    "        flag=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Generate text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_text(model, length, vocab_size, ix_to_char):\n",
    "    # starting with random character\n",
    "    ix = [np.random.randint(vocab_size)]\n",
    "    y_char = [ix_to_char[ix[-1]]]\n",
    "    X = np.zeros((1, length, vocab_size))\n",
    "    for i in range(length):\n",
    "        # appending the last predicted character to sequence\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        print(ix_to_char[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(ix_to_char[ix[-1]])\n",
    "    if not os.path.exists(out_path+\"generate/\"):\n",
    "        os.makedirs(out_path+\"generate/\")\n",
    "    with open(out_path+\"generate/output.txt\",\"w\") as f:\n",
    "        f.write(('').join(y_char))\n",
    "    return ('').join(y_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "out = save_text(model, 1500, VOCAB_SIZE, indices_char)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
