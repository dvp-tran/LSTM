{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Dropout,TimeDistributed\n",
    "from keras.layers import LSTM,SimpleRNN\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os, os.path\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- ### Check CPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'/gpu:0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Toy examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1. Load and convert data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Example script to generate text from Nietzsche's writings.\n",
    "At least 20 epochs are required before the generated text\n",
    "starts sounding coherent.\n",
    "It is recommended to run this script on GPU, as recurrent\n",
    "networks are quite computationally intensive.\n",
    "If you try this script on new data, make sure your corpus\n",
    "has at least ~100k characters. ~1M is better.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done concatenating file : httpwww.imsdb.comscriptsA-Most-Violent-Year.html.txt\n",
      "Done concatenating file : httpwww.imsdb.comscriptsAlien-3.html.txt\n",
      "Done concatenating file : httpwww.imsdb.comscripts48-Hrs..html.txt\n",
      "Done concatenating file : httpwww.imsdb.comscriptsAlien.html.txt\n",
      "Done concatenating file : httpwww.imsdb.comscriptsAmerican-Shaolin-King-of-Kickboxers-II.html.txt\n",
      "Done concatenating file : httpwww.imsdb.comscriptsAustin-Powers---International-Man-of-Mystery.html.txt\n",
      "Done concatenating file : httpwww.imsdb.comscriptsAustin-Powers---The-Spy-Who-Shagged-Me.html.txt\n",
      "Done concatenating file : httpwww.imsdb.comscriptsArmy-of-Darkness.html.txt\n",
      "Done concatenating file : httpwww.imsdb.comscriptsArctic-Blue.html.txt\n",
      "Done concatenating file : httpwww.imsdb.comscriptsArmageddon.html.txt\n",
      "Done concatenating file : httpwww.imsdb.comscriptsStar-Wars-The-Force-Awakens.html.txt\n",
      "Done concatenating file : httpwww.imsdb.comscriptsJackie-Brown.html.txt\n",
      "Done concatenating file : httpwww.imsdb.comscriptsAssassins.html.txt\n",
      "Done concatenating file : httpwww.imsdb.comscriptsAlien-vs.-Predator.html.txt\n",
      "Done concatenating file : httpwww.imsdb.comscriptsBad-Country.html.txt\n"
     ]
    }
   ],
   "source": [
    "#Load and concatenate files:\n",
    "\n",
    "DIR=\"../../LSTM/data/all_script/action/raw/\"\n",
    "all_files = [f for f in listdir(DIR) if isfile(join(DIR, f))]\n",
    "\n",
    "#choose how many files to concatenate:\n",
    "nb_files=15\n",
    "if nb_files>len(all_files):\n",
    "    nb_files=len(all_files)\n",
    "    \n",
    "    \n",
    "out_path=\"hollywood/data/\"\n",
    "if not os.path.exists(out_path):\n",
    "    os.makedirs(out_path)\n",
    "if not os.path.exists(out_path+\"input/\"):\n",
    "    os.makedirs(out_path+\"input/\")\n",
    "    \n",
    "with open(out_path+'input/input_action.txt', 'w') as outfile:\n",
    "    for fname in all_files[0:nb_files]:\n",
    "        with open(DIR+fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)\n",
    "        print (\"Done concatenating file : %s\" %fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 2627888\n",
      "total chars: 100\n"
     ]
    }
   ],
   "source": [
    "#load file\n",
    "file_name=out_path+'input/input_action.txt'\n",
    "text = open(file_name).read()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "VOCAB_SIZE = len(chars)\n",
    "print('total chars:',VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** The RNN takes in input numerical data hence the necessity to convert strings into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating mapping between indexes and characters\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weâ€™re gonna use Keras to create and train our Network, so we must convert the data into this form: (number_of_sequences, length_of_sequence, number_of_features).\n",
    "- nb of features = length of the char array\n",
    "- length of sequence = batch size\n",
    "- nb of sequence = len(data) divided by batch size.\n",
    "\n",
    "**Warning : ** target sequence is setted by shifting the source/input sequence by one character with both having the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.96 s, sys: 220 ms, total: 2.18 s\n",
      "Wall time: 2.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "SEQ_LENGTH=100\n",
    "#Build three dimensional arrays\n",
    "X = np.zeros((len(text)/SEQ_LENGTH, SEQ_LENGTH, VOCAB_SIZE)) #input\n",
    "y = np.zeros((len(text)/SEQ_LENGTH, SEQ_LENGTH, VOCAB_SIZE)) #target\n",
    "\n",
    "#Build sequences\n",
    "for i in range(0, len(text)/SEQ_LENGTH):\n",
    "    X_sequence = text[i*SEQ_LENGTH:(i+1)*SEQ_LENGTH]\n",
    "    X_sequence_ix = [char_indices[value] for value in X_sequence]\n",
    "    input_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n",
    "    for j in range(SEQ_LENGTH):\n",
    "        input_sequence[j][X_sequence_ix[j]] = 1.\n",
    "    X[i] = input_sequence\n",
    "\n",
    "    y_sequence = text[i*SEQ_LENGTH+1:(i+1)*SEQ_LENGTH+1]\n",
    "    y_sequence_ix = [char_indices[value] for value in y_sequence]\n",
    "    target_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n",
    "    for j in range(SEQ_LENGTH):\n",
    "        target_sequence[j][y_sequence_ix[j]] = 1.\n",
    "    y[i] = target_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2. Build the network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HIDDEN_DIM= 500 #500\n",
    "LAYER_NUM = 2\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(LAYER_NUM - 1):\n",
    "    model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_text(model, length, vocab_size, ix_to_char):\n",
    "    # starting with random character\n",
    "    ix = [np.random.randint(vocab_size)]\n",
    "    y_char = [ix_to_char[ix[-1]]]\n",
    "    X = np.zeros((1, length, vocab_size))\n",
    "    for i in range(length):\n",
    "        # appending the last predicted character to sequence\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        print(ix_to_char[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(ix_to_char[ix[-1]])\n",
    "    return ('').join(y_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9x::::llll::lllmï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½(BB(BMtWWMWWWï¿½RRP{{{{nn{{\t\tOOO\t\t%55O%+++ssss1111pwwpp1}LLLzzzz66ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Rmm"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'9x::::llll::lllm\\xa9\\xa9\\xa9\\xa9\\xa9\\xa9\\xa2\\xa2\\xa2\\xa2(BB(BMtWWMWWW\\x80RRP{{{{nn{{\\t\\tOOO\\t\\t%55O%+++ssss1111pwwpp1}LLLzzzz66\\xc2\\xc2\\x80\\x80\\x80\\x80\\x80Rmm\\x80'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate some sample before training to know how bad it is!\n",
    "generate_text(model, 100, VOCAB_SIZE, indices_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Train network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "Starting at iteration : 0\n",
      "\n",
      "\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda2/lib/python2.7/site-packages/keras/models.py:834: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "108s - loss: 2.4128\n",
      "N                                                                                                   \n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "107s - loss: 1.5932\n",
      "S                                                                                                   \n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "113s - loss: 1.3537\n",
      "he the stare and the stare and the \n",
      "the stare and the stare and the stare and the \n",
      "the stare and the\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "100s - loss: 1.2177\n",
      "S the door and the stare and the stares and the \n",
      "                         the door and the man and t\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "99s - loss: 1.1290\n",
      "(                                                                                                   \n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "110s - loss: 1.0659\n",
      "<b>                                                                                                 \n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "107s - loss: 1.0174\n",
      "1\n",
      "</b>                         I have a lot of my money.\n",
      "\n",
      "<b>                                     AU\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "104s - loss: 0.9786\n",
      "X STARISH - DAY\n",
      "</b>\n",
      "The control stare at the stare and stares at the station of the control of the \n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "104s - loss: 0.9455\n",
      "ON THE TERRICOR - DAY\n",
      "</b>\n",
      "The confused of the control of the control of the control of the control \n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "113s - loss: 0.9160\n",
      "b>                                                                                                  \n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "103s - loss: 0.8890\n",
      "ï¿½ the shotgun. The shoulder of the \n",
      "               the corner of the shotgun and shows the shoet and\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "104s - loss: 0.8652\n",
      "9\t\t\t\t\t\t\t\t\t\t\t\t 242\n",
      "</b>\t   rides at the barrel of the car.  He stares at the controls.\n",
      "<b>482\tASH\t\t\t\t\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "118s - loss: 0.8418\n",
      "4\n",
      "</b>\t   Ash and then they are starting to stare at the floor.\n",
      "<b>333\tASH\t\t\t\t\t\t\t\t\t\t\t\t 233\n",
      "</b>\t   r\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "122s - loss: 0.8195\n",
      "\n",
      "<b>                                     AUSTIN\n",
      "</b>                         I don't know what you w\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "112s - loss: 0.7979\n",
      "y the same place.  And then \n",
      "the ground are sitting at the trap of the temple. \n",
      "\n",
      "<b>INT. SHUTTLE FRE\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "115s - loss: 0.7768\n",
      "way the floor.\n",
      "<b>   \n",
      "</b><b>                       JACKIE\n",
      "</b>             I know you don't know a \n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "100s - loss: 0.7562\n",
      "ï¿½ see the stairs and the ship strips of the stairs and the ship \n",
      "strips of the shadow computer scree\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "107s - loss: 0.7364\n",
      "8\n",
      "</b>                         \n",
      "          ABEL is still at the truck and gets out of the courtyard\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "101s - loss: 0.7166\n",
      "g the trappers start to stand up the phone.\n",
      "\n",
      "<b>                                     DR. EVIL\n",
      "</b>  \n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "111s - loss: 0.6975\n",
      "me to show you as he\n",
      "               sees it on the courtyard.\n",
      "\n",
      "<b>                             ELECT\n",
      "\n",
      "Iteration nb : 20\n",
      "\n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n",
      "116s - loss: 0.6787\n",
      "FINN\n",
      "</b>           They're obviously underneeted to\n",
      "           the ship to you any move.\n",
      "       \n",
      "\n",
      "--------------------\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-763051ad6308>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mnb_iteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGENERATE_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/paul/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    851\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/paul/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/paul/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/paul/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2101\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2103\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/paul/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/paul/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/paul/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/paul/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/paul/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#batch size equals to seq length here\n",
    "BATCH_SIZE=100\n",
    "#len of desired output\n",
    "GENERATE_LENGTH=100\n",
    "DIR=out_path+\"weights/weight_attempt_s01/\"\n",
    "\n",
    "try:\n",
    "    onlyfiles = [f for f in listdir(DIR) if isfile(join(DIR, f))]\n",
    "    nb_files = len(onlyfiles)\n",
    "    name_template = onlyfiles[0][0:15]\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    if not os.path.exists(DIR):\n",
    "        os.makedirs(DIR)\n",
    "    nb_files=0\n",
    "\n",
    "if nb_files>0:\n",
    "    nb_iteration=nb_files*20\n",
    "    model.load_weights(DIR+onlyfiles[0][0:21]+str(nb_iteration)+'.hdf5')\n",
    "else:\n",
    "    nb_iteration=0\n",
    "    \n",
    "print(\"Starting at iteration : %s\" %nb_iteration)\n",
    "while True:\n",
    "    print('\\n')\n",
    "    print('-'*20)\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, verbose=2, nb_epoch=1)\n",
    "    nb_iteration += 1\n",
    "    generate_text(model, GENERATE_LENGTH,VOCAB_SIZE, indices_char)\n",
    "    if nb_iteration % 20 == 0:\n",
    "        print(\"\\n\\nIteration nb : %s\" %nb_iteration)\n",
    "        model.save_weights(DIR+'checkpoint_{}_epoch_{}.hdf5'.format(HIDDEN_DIM, nb_iteration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Generate text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_text(model, length, vocab_size, ix_to_char):\n",
    "    # starting with random character\n",
    "    ix = [np.random.randint(vocab_size)]\n",
    "    y_char = [ix_to_char[ix[-1]]]\n",
    "    X = np.zeros((1, length, vocab_size))\n",
    "    for i in range(length):\n",
    "        # appending the last predicted character to sequence\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        print(ix_to_char[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(ix_to_char[ix[-1]])\n",
    "    if not os.path.exists(out_path+\"generate/\"):\n",
    "        os.makedirs(out_path+\"generate/\")\n",
    "    with open(out_path+\"generate/output.txt\",\"w\") as f:\n",
    "        f.write(('').join(y_char))\n",
    "    return ('').join(y_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f the first time, and the door opens and weapons into the\n",
      "     corner.\n",
      "\n",
      "     He looks at her.\n",
      "\n",
      "<b>                             RATH\n",
      "</b>               Yes.\n",
      "\n",
      "<b>                             RATH\n",
      "</b>               I was the continue.\n",
      "\n",
      "<b>                             RATH\n",
      "</b>               I was the continue.\n",
      "\n",
      "<b>                             RATH\n",
      "</b>               I was the continue.\n",
      "\n",
      "<b>                             RATH\n",
      "</b>               I was the continue.\n",
      "\n",
      "<b>                             RATH\n",
      "</b>               I was the continue.\n",
      "\n",
      "<b>                             RATH\n",
      "</b>               I was the continue.\n",
      "\n",
      "<b>                             RATH\n",
      "</b>               I was the continue.\n",
      "\n",
      "<b>                             RATH\n",
      "</b>               I was the continue.\n",
      "\n",
      "<b>                             RATH\n",
      "</b>               I was the continue.\n",
      "\n",
      "<b>                             RATH\n",
      "</b>               I was the continue.\n",
      "\n",
      "<b>                             RATH\n",
      "</b>               I was the continue.\n",
      "\n",
      "<b>                             RATH\n",
      "</b>               I was the continue.\n",
      "\n",
      "<b>                             RATH\n",
      "</b>               I was the continue.\n",
      "\n",
      "<b>                             RATH\n",
      "</b>               I was the continue.\n",
      "\n",
      "<b>                             RATH\n",
      "</b>               I was the continue.\n",
      "\n",
      "<b>                             RATH\n",
      "</b>               I was the continue.\n",
      "\n",
      "<b>                             RATH\n",
      "</b>               I was the continue.\n",
      "\n",
      "<b>                             RACPU times: user 6min 49s, sys: 6.46 s, total: 6min 55s\n",
      "Wall time: 6min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "out = save_text(model, 1500, VOCAB_SIZE, indices_char)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
